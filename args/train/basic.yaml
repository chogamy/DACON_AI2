output_dir: ./outputs #str
overwrite_output_dir: false # bool = False
do_train: false # bool = False
do_eval: false # bool = False
do_predict: false #bool = False
evaluation_strategy: 'no' # Union = 'no'
prediction_loss_only: false # bool = False
per_device_train_batch_size: 8 # int = 8
per_device_eval_batch_size: 8 #int = 8
per_gpu_train_batch_size: null # Optional = null
per_gpu_eval_batch_size: null # Optional = null
gradient_accumulation_steps: 1 # int = 1
eval_accumulation_steps: null #Optional = null
eval_delay: 0 # Optional = 0
learning_rate: 5e-05 # float = 5e-05
weight_decay: 0.0 # float = 0.0
adam_beta1: 0.9 #float = 0.9
adam_beta2: 0.999 # float = 0.999
adam_epsilon: 1e-08 # float = 1e-08
max_grad_norm: 1.0 # float = 1.0
num_train_epochs: 3.0 # float = 3.0
max_steps: -1 # int = -1
lr_scheduler_type: 'lienar' # Union = 'linear'
lr_scheduler_kwargs: <factory> # Optional = <factory>
warmup_ratio: 0.0 # float = 0.0
warmup_steps: 0 # int = 0
log_level: 'passive' # Optional = 'passive'
log_level_replica: 'warning' # Optional = 'warning'
log_on_each_node: true # bool = True
logging_dir: null # Optional = null
logging_strategy: 'steps' # Union = 'steps'
logging_first_step: false # bool = False
logging_steps: 500 # float = 500
logging_nan_inf_filter: true # bool = True
save_strategy: 'steps' # Union = 'steps'
save_steps: 500 # float = 500
save_total_limit: null # Optional = null
save_safetensors: true # Optional = True
save_on_each_node: false # bool = False
save_only_model: false # bool = False
no_cuda: false # bool = False
use_cpu: false # bool = False
use_mps_device: false # bool = False
seed: 42 #  int = 42
data_seed: null # Optional = null
jit_mode_eval: false # bool = False
use_ipex: false 3 bool = False
bf16: false # bool = False
fp16: false # bool = False
fp16_opt_level: 'O1' # str = 'O1'
half_precision_backend: 'auto' #str = 'auto'
bf16_full_eval: false # bool = False
fp16_full_eval: false bool = False
tf32: null # Optional = null
local_rank: -1 #int = -1
ddp_backend: null # Optional = null
tpu_num_cores: null # Optional = null
tpu_metrics_debug: false #bool = False
debug: '' #Union = ''
dataloader_drop_last: false # bool = False
eval_steps: null # Optional = null
dataloader_num_workers: 0 # int = 0
past_index: -1 #int = -1
run_name: null # Optional = null
disable_tqdm: null  #Optional = null
remove_unused_columns: true # Optional = True
label_names: null # Optional = null
load_best_model_at_end: false # Optional = False
metric_for_best_model: null # Optional = null
greater_is_better: null # Optional = null
ignore_data_skip: false #bool = False
fsdp: '' # Union = ''
fsdp_min_num_params: 0 #  int = 0
fsdp_config: null # Optional = null
fsdp_transformer_layer_cls_to_wrap: null # Optional = null
deepspeed: null # Optional = null
label_smoothing_factor: 0.0 # float = 0.0
optim: 'adamw_torch' #Union = 'adamw_torch'
optim_args: null 3 Optional = null
adafactor: false # bool = False
group_by_length: false # bool = False
length_column_name: 'length' # Optional = 'length'
report_to: null # Optional = null
ddp_find_unused_parameters: null #  Optional = null
ddp_bucket_cap_mb: null # Optional = null
ddp_broadcast_buffers: null # Optional = null
dataloader_pin_memory: true # bool = True
dataloader_persistent_workers: false # bool = False
skip_memory_metrics: true # bool = True
use_legacy_prediction_loop: false # bool = False
push_to_hub: false # bool = False
resume_from_checkpoint: null #Optional = null
hub_model_id: null # Optional = null
hub_strategy: 'every_save' #Union = 'every_save'
hub_token: null # Optional = null
hub_private_repo: false # bool = False
hub_always_push: false # bool = False
gradient_checkpointing: false # bool = False
gradient_checkpointing_kwargs: null # Optional = null
include_inputs_for_metrics: false # bool = False
fp16_backend: 'auto' # str = 'auto'
push_to_hub_model_id: null #Optional = null
push_to_hub_organization: null # Optional = null
push_to_hub_token: null # Optional = null
mp_parameters: '' #str = ''
auto_find_batch_size: false #  bool = False
full_determinism: false # bool = False
torchdynamo: null # Optional = null
ray_scope: 'last' # Optional = 'last'
ddp_timeout: 1800 #Optional = 1800
torch_compile: false # bool = False
torch_compile_backend: null # Optional = null
torch_compile_mode: null # Optional = null
dispatch_batches: null # Optional = null
split_batches: false # Optional = False
include_tokens_per_second: false #  Optional = False
include_num_input_tokens_seen: false # Optional = False
neftune_noise_alpha: null # float = null